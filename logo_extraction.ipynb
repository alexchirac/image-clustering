{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies\n",
    "\n",
    "The following cell installs all the required dependencies for the code.  \n",
    "\n",
    "The code is made on **Python 3.13.2**. On older versions, it might be deprecated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/student/.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /home/student/.local/lib/python3.10/site-packages (18.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastparquet in /home/student/.local/lib/python3.10/site-packages (2024.11.0)\n",
      "Requirement already satisfied: numpy in /home/student/.local/lib/python3.10/site-packages (from fastparquet) (1.23.5)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/student/.local/lib/python3.10/site-packages (from fastparquet) (2.9.1)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/student/.local/lib/python3.10/site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: fsspec in /home/student/.local/lib/python3.10/site-packages (from fastparquet) (2024.6.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/student/.local/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fuzzywuzzy in /home/student/.local/lib/python3.10/site-packages (0.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cairosvg in /home/student/.local/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: defusedxml in /home/student/.local/lib/python3.10/site-packages (from cairosvg) (0.7.1)\n",
      "Requirement already satisfied: cssselect2 in /home/student/.local/lib/python3.10/site-packages (from cairosvg) (0.8.0)\n",
      "Requirement already satisfied: cairocffi in /home/student/.local/lib/python3.10/site-packages (from cairosvg) (1.7.1)\n",
      "Requirement already satisfied: tinycss2 in /home/student/.local/lib/python3.10/site-packages (from cairosvg) (1.4.0)\n",
      "Requirement already satisfied: pillow in /home/student/.local/lib/python3.10/site-packages (from cairosvg) (11.1.0)\n",
      "Requirement already satisfied: cffi>=1.1.0 in /home/student/.local/lib/python3.10/site-packages (from cairocffi->cairosvg) (1.17.1)\n",
      "Requirement already satisfied: webencodings in /usr/lib/python3/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /home/student/.local/lib/python3.10/site-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: pillow 11.1.0\n",
      "Uninstalling pillow-11.1.0:\n",
      "  Successfully uninstalled pillow-11.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Usage:   \n",
      "  /bin/python3 -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /bin/python3 -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /bin/python3 -m pip install [options] [-e] <vcs project url> ...\n",
      "  /bin/python3 -m pip install [options] [-e] <local project path> ...\n",
      "  /bin/python3 -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/student/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/student/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/student/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/student/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/student/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/student/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /home/student/.local/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/student/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/student/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/student/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/student/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/student/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/student/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/student/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/student/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/student/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/student/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/student/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/student/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/student/.local/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas \n",
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install requests beautifulsoup4\n",
    "!pip install fuzzywuzzy\n",
    "!pip install cairosvg\n",
    "!pip uninstall -y pillow\n",
    "!pip install --upgrade -y pillow\n",
    "!pip install torch torchvision\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update this for parallel processing.\n",
    "\n",
    "For more powerful computers this number can be set higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing CSV File\n",
    "\n",
    "1. **Extract the CSV file** and load its contents.  \n",
    "\n",
    "2. **Add a secondary column** to keep track of the domains that have been parsed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     domain  extracted\n",
      "0                         stanbicbank.co.zw      False\n",
      "1                            astrazeneca.ua      False\n",
      "2               autosecuritas-ct-seysses.fr      False\n",
      "3                                    ovb.ro      False\n",
      "4     mazda-autohaus-hellwig-hoyerswerda.de      False\n",
      "...                                     ...        ...\n",
      "4379                              synlab.ec      False\n",
      "4380                            ccusa.co.za      False\n",
      "4381               aamcolawrencevillega.com      False\n",
      "4382     mazda-autohaus-born-ludwigslust.de      False\n",
      "4383                     savethechildren.ca      False\n",
      "\n",
      "[4384 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('logos.snappy.parquet')\n",
    "\n",
    "df.to_csv('data.csv', index=False)\n",
    "\n",
    "df['extracted'] = False\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step Logo Extraction\n",
    "\n",
    "- **Used Clearbit** for the initial logo extraction.  \n",
    "\n",
    "- **Achieved an 84.7% initial extraction success rate** for logos.  \n",
    "\n",
    "- **Implemented `ThreadPoolExecutor`** for parallel processing of the domains.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def proc_domain(data):\n",
    "    index, domain = data\n",
    "    \n",
    "    url = f\"https://logo.clearbit.com/{domain}\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Create images directory if it doesn't exist\n",
    "            if not os.path.exists('images'):\n",
    "                os.makedirs('images')\n",
    "                \n",
    "            with open(f\"images/{index}.png\", 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return (index, True)\n",
    "        else:\n",
    "            return (index, False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {domain}: {str(e)}\")\n",
    "        return (index, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading logos: 100%|██████████| 4384/4384 [02:49<00:00, 30.92logo/s, success=3713/4384]"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the data for processing\n",
    "domains = list(enumerate(df['domain']))\n",
    "total = len(domains)\n",
    "\n",
    "# Initialize a progress bar\n",
    "progress_bar = tqdm(total=total, desc=\"Downloading logos\", unit=\"logo\")\n",
    "\n",
    "# Track successful downloads\n",
    "successful = 0\n",
    "\n",
    "# Using ThreadPoolExecutor for parallel requests\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks and get futures\n",
    "    futures = [executor.submit(proc_domain, data) for data in domains]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in futures:\n",
    "        try:\n",
    "            index, success = future.result()\n",
    "            if success:\n",
    "                df.at[index, 'extracted'] = True\n",
    "                successful += 1\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\"success\": f\"{successful}/{total}\"})\n",
    "        except Exception as e:\n",
    "            progress_bar.update(1)\n",
    "            print(f\"Error processing task: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stanbicbank.co.zw</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>astrazeneca.ua</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>autosecuritas-ct-seysses.fr</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ovb.ro</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mazda-autohaus-hellwig-hoyerswerda.de</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379</th>\n",
       "      <td>synlab.ec</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380</th>\n",
       "      <td>ccusa.co.za</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>aamcolawrencevillega.com</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>mazda-autohaus-born-ludwigslust.de</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4383</th>\n",
       "      <td>savethechildren.ca</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4384 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     domain  extracted\n",
       "0                         stanbicbank.co.zw       True\n",
       "1                            astrazeneca.ua       True\n",
       "2               autosecuritas-ct-seysses.fr       True\n",
       "3                                    ovb.ro       True\n",
       "4     mazda-autohaus-hellwig-hoyerswerda.de       True\n",
       "...                                     ...        ...\n",
       "4379                              synlab.ec      False\n",
       "4380                            ccusa.co.za       True\n",
       "4381               aamcolawrencevillega.com       True\n",
       "4382     mazda-autohaus-born-ludwigslust.de      False\n",
       "4383                     savethechildren.ca       True\n",
       "\n",
       "[4384 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy_df = df.copy(deep=True)\n",
    "copy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Step of the Logo Extraction\n",
    "\n",
    "- **Used Gemini API** to find the logo's URL in the page's header.  \n",
    "\n",
    "- **Also used `ThreadPoolExecutor`** for the Gemini API calls.  \n",
    "\n",
    "- **Downloaded** `.jpg`, `.png`, and `.svg` files.  \n",
    "\n",
    "- **Converted `.svg` files to `.png`** for later use in feature extraction.  \n",
    "\n",
    "- **Managed to achieve an additional 9% extraction rate** with this approach.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "def clean_html(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and cleans only the content inside the <header> tag by removing\n",
    "    <script> comment tags.\n",
    "\n",
    "    Args:\n",
    "        html (str): Raw HTML content to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned content inside <header>, or an empty string if <header> is missing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Extract the header tag\n",
    "        header = soup.header\n",
    "        if not header:\n",
    "            return \"\"  # Return empty string if no <header> exists\n",
    "\n",
    "        # Remove <script> and <style> tags inside the header\n",
    "        for tag in header([\"script\"]):\n",
    "            tag.decompose()  # Removes tag and its content\n",
    "\n",
    "        # Remove comments inside the header\n",
    "        for comment in header.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        # Return cleaned header HTML\n",
    "        return header.prettify()\n",
    "\n",
    "    except Exception:\n",
    "        return \"\"  # Return empty string if cleaning fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_png(url, save_path):\n",
    "    # Send GET request to the URL\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Save the file\n",
    "    with open(save_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "            \n",
    "    return True\n",
    "\n",
    "def proc_data(url, idx):\n",
    "    if '.svg' in url:\n",
    "        save_path = f'./images/{idx}.svg'\n",
    "    elif '.png' in url:\n",
    "        save_path = f'./images/{idx}.png'\n",
    "    elif '.jpg' in url:\n",
    "        save_path = f'./images/{idx}.jpg'\n",
    "        \n",
    "    download_png(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_clients import call_gemini\n",
    "\n",
    "def process_nth_domain(n):\n",
    "    if copy_df['extracted'][n] == False:\n",
    "        try:\n",
    "            url = f\"https://www.{copy_df['domain'][n]}\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                html = response.text\n",
    "                header = clean_html(html)\n",
    "                \n",
    "                response = call_gemini(f\"\"\" Tell me what is the logo url for this header: {header}. \n",
    "                                          The domain is {copy_df['domain'][n]}. \n",
    "                                          I want you to find only the url, no aditional information.\n",
    "                                          The url should look like this: https://www.example.com/logo.png.\n",
    "                                          If you can't find 'https' add it.\n",
    "                                          If you can't find the logo url, just type 'NotFound'.\n",
    "                                          \n",
    "                                          It is MANDATORY that if a url is found it is returned with 'https://' in the beginning\n",
    "                                          It is MANDATORY that if a url is not found the result is 'NotFound' exactly like this\"\"\")[:-1]\n",
    "                \n",
    "                if response.lower() == 'notfound':\n",
    "                    return\n",
    "                \n",
    "                proc_data(response, n)\n",
    "                \n",
    "                copy_df.loc[n, 'extracted'] = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url} - {str(e)}\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ThreadPoolExecutor for parallel requests\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    executor.map(process_nth_domain, range(len(copy_df['domain'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ./images/3866.svg: The SVG size is undefined\n",
      "Error processing ./images/934.svg: not well-formed (invalid token): line 507, column 54\n",
      "Error processing ./images/495.svg: mismatched tag: line 7, column 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import html\n",
    "import cairosvg\n",
    "\n",
    "def clean_svg_for_parsing(svg_file_path):\n",
    "    \"\"\"\n",
    "    Clean SVG file by replacing HTML entities with proper XML entities\n",
    "    \n",
    "    Parameters:\n",
    "    svg_file_path (str): Path to the input SVG file\n",
    "    output_file_path (str): Path where the cleaned SVG will be saved (optional)\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to the cleaned SVG file\n",
    "    \"\"\"\n",
    "    base, ext = os.path.splitext(svg_file_path)\n",
    "    output_file_path = f\"{base}_cleaned{ext}\"\n",
    "    \n",
    "    try:\n",
    "        # Read the SVG file\n",
    "        with open(svg_file_path, 'r', encoding='utf-8') as file:\n",
    "            svg_content = file.read()\n",
    "        \n",
    "        # Find and replace HTML entities\n",
    "        def replace_entity(match):\n",
    "            entity = match.group(1)\n",
    "            # Convert HTML entity to its Unicode character\n",
    "            return html.unescape(f\"&{entity};\")\n",
    "        \n",
    "        # Replace entities like &aacute; with their Unicode equivalents\n",
    "        cleaned_content = re.sub(r'&([a-zA-Z]+);', replace_entity, svg_content)\n",
    "        \n",
    "        # Write the cleaned content to a new file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_content)\n",
    "        \n",
    "        return output_file_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning SVG file {svg_file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get all files in the images directory\n",
    "filenames = os.listdir('./images')\n",
    "\n",
    "for filename in filenames:\n",
    "    if '.svg' in filename:\n",
    "        prefix = filename.split('.')[0]\n",
    "        svg_path = f'./images/{prefix}.svg'\n",
    "        png_path = f'./images/{prefix}.png'\n",
    "        \n",
    "        try:\n",
    "            # Clean the SVG file first\n",
    "            cleaned_svg = clean_svg_for_parsing(svg_path)\n",
    "            \n",
    "            if cleaned_svg:\n",
    "                # Convert the cleaned SVG to PNG\n",
    "                cairosvg.svg2png(url=cleaned_svg, write_to=png_path)\n",
    "                \n",
    "                # Remove the temporary cleaned file\n",
    "                os.remove(cleaned_svg)\n",
    "                \n",
    "                # Remove the original SVG files\n",
    "                os.remove(svg_path)\n",
    "            else:\n",
    "                print(f\"Failed to clean {svg_path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            os.remove(svg_path)\n",
    "            os.remove(cleaned_svg)\n",
    "            print(f\"Error processing {svg_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "- **Used EfficientNetB5** for feature extraction.  \n",
    "\n",
    "- **Removed the last layer** to take only the feature vectors, without classification.  \n",
    "\n",
    "- **Used `ThreadPoolExecutor`** for faster feature processing.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class LogoNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A specialized neural network for logo recognition with:\n",
    "    1. Enhanced local feature extraction\n",
    "    2. Multi-scale processing\n",
    "    3. Shape-aware attention\n",
    "    4. Rotation and scale invariance\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=512):\n",
    "        super(LogoNet, self).__init__()\n",
    "        \n",
    "        # Base feature extractor (EfficientNet is good for logos due to better edge detection)\n",
    "        # Could use ResNet50 or other backbones too\n",
    "        self.backbone = models.efficientnet_b5(weights=models.EfficientNet_B5_Weights.DEFAULT)\n",
    "        backbone_out_features = 2048  # EfficientNet-B5 output features\n",
    "        \n",
    "        # Remove classifier head\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        \n",
    "        # Multi-scale feature extraction (helps with logos of different scales)\n",
    "        self.conv_1x1 = nn.Conv2d(backbone_out_features, 256, kernel_size=1)\n",
    "        self.conv_3x3 = nn.Conv2d(backbone_out_features, 256, kernel_size=3, padding=1)\n",
    "        self.conv_5x5 = nn.Conv2d(backbone_out_features, 256, kernel_size=5, padding=2)\n",
    "        \n",
    "        # Shape-aware attention module\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(768, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 768, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Global context module\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Feature embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(768, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract base features\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        feat_1x1 = self.conv_1x1(x)\n",
    "        feat_3x3 = self.conv_3x3(x)\n",
    "        feat_5x5 = self.conv_5x5(x)\n",
    "        \n",
    "        # Concatenate multi-scale features\n",
    "        multi_scale_features = torch.cat([feat_1x1, feat_3x3, feat_5x5], dim=1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = self.attention(multi_scale_features)\n",
    "        attended_features = multi_scale_features * attention_weights\n",
    "        \n",
    "        # Global pooling and flatten\n",
    "        x = self.global_pool(attended_features)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Get embedding\n",
    "        embedding = self.embedding(x)\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LogoNet()\n",
    "model.eval()\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Example transform for inference\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract logo features using pretrained model\n",
    "def extract_logo_features(image_path, model=model, transform=inference_transform):\n",
    "    \"\"\"Extract features from a logo image using a specialized logo model\"\"\"\n",
    "    try:\n",
    "        # Open image\n",
    "        image = Image.open(f\"./images/{image_path}\").convert('RGB')\n",
    "        \n",
    "        # Apply transformations\n",
    "        image = transform(image).unsqueeze(0)\n",
    "        \n",
    "        # Move to the same device as model\n",
    "        device = next(model.parameters()).device\n",
    "        image = image.to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = model(image)\n",
    "        \n",
    "        # Return normalized features\n",
    "        return F.normalize(features, p=2, dim=1).view(1, -1), image_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 3111.png: cannot identify image file './images/3111.png'\n",
      "Error processing 1865.png: cannot identify image file './images/1865.png'\n",
      "Error processing 4375.png: cannot identify image file './images/4375.png'\n",
      "Error processing 2006.png: cannot identify image file './images/2006.png'\n",
      "Error processing 253.png: cannot identify image file './images/253.png'\n",
      "Error processing 2981.png: cannot identify image file './images/2981.png'\n",
      "Error processing 3382.png: cannot identify image file './images/3382.png'\n",
      "Error processing 2803.png: cannot identify image file './images/2803.png'\n",
      "Error processing 517.png: cannot identify image file './images/517.png'\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "filenames = os.listdir('./images')\n",
    "features = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    parallel_features = executor.map(extract_logo_features, filenames)\n",
    "    \n",
    "features = [f for f in parallel_features if f is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-Force Clustering\n",
    "\n",
    "- **Defined a `SIMILARITY_THRESHOLD`** that can be adjusted based on preference.  \n",
    "\n",
    "- **A higher `THRESHOLD` results in more similar logos per cluster but also increases the number of clusters.**  \n",
    "\n",
    "- **Brute-force approach:** Each logo is compared against all logos in existing clusters to ensure optimal placement.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMILARITY_THRESHOLD = 0.7  # Adjust this threshold based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering Images: 100%|██████████| 3949/3949 [00:47<00:00, 84.01image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "clusters = []\n",
    "image_clusters = []\n",
    "\n",
    "# Total number of features to process\n",
    "total_features = len(features)\n",
    "\n",
    "# Iterate through features with a progress bar\n",
    "for feature in tqdm(features, desc=\"Clustering Images\", unit=\"image\", total=total_features):\n",
    "    if feature is None:\n",
    "        continue\n",
    "    feature, filename = feature\n",
    "    \n",
    "    best_cluster = -1\n",
    "    best_cluster_raport = 0\n",
    "\n",
    "    for i in range(len(clusters)):\n",
    "        cluster = clusters[i]\n",
    "        close = 0\n",
    "        far = 0\n",
    "\n",
    "        for image_feature in cluster:\n",
    "            similarity = F.cosine_similarity(feature, image_feature, dim=1).item()\n",
    "            \n",
    "            if similarity < SIMILARITY_THRESHOLD:\n",
    "                far += 1\n",
    "            else:\n",
    "                close += 1\n",
    "\n",
    "        if close / (close + far) > 0.8 and close / (close + far) > best_cluster_raport:\n",
    "            best_cluster = i\n",
    "            best_cluster_raport = close / (close + far)\n",
    "\n",
    "    if best_cluster == -1:\n",
    "        clusters.append([feature])\n",
    "        image_clusters.append([filename])\n",
    "    else:\n",
    "        clusters[best_cluster].append(feature)\n",
    "        image_clusters[best_cluster].append(filename)\n",
    "        \n",
    "print(f\"Number of clusters: {len(clusters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and JSON Generation\n",
    "\n",
    "- **Grouped all images** based on the clusters formed in the previous step.  \n",
    "\n",
    "- **Generated a JSON file** mapping domains to their respective clusters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "for i in range(len(image_clusters)):\n",
    "    if not os.path.exists(f\"./clusters/cluster{i}\"):\n",
    "        os.makedirs(f\"./clusters/cluster{i}\")\n",
    "    for image_path in image_clusters[i]:\n",
    "        os.rename(f\"./images/{image_path}\", f\"./clusters/cluster{i}/{image_path}\")\n",
    "        \n",
    "df = pd.read_csv('data.csv')\n",
    "domain_clusters = [[df['domain'][int(y.split('.')[0])] for y in x]for x in image_clusters]\n",
    "json.dump(domain_clusters, open('clusters.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Define source and destination folders\n",
    "# source_folder = \"./clusters\"  # Folder containing cluster subfolders\n",
    "# destination_folder = \"./images\"  # Target folder\n",
    "\n",
    "# # Ensure destination folder exists\n",
    "# os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# # Loop through all subfolders in 'clusters'\n",
    "# for subfolder in os.listdir(source_folder):\n",
    "#     subfolder_path = os.path.join(source_folder, subfolder)\n",
    "\n",
    "#     # Check if it's a directory\n",
    "#     if os.path.isdir(subfolder_path):\n",
    "#         # Move each file inside the subfolder\n",
    "#         for filename in os.listdir(subfolder_path):\n",
    "#             file_path = os.path.join(subfolder_path, filename)\n",
    "            \n",
    "#             if os.path.isfile(file_path):  # Ensure it's a file\n",
    "#                 shutil.move(file_path, os.path.join(destination_folder, filename))\n",
    "                \n",
    "#         os.rmdir(subfolder_path)\n",
    "\n",
    "# print(\"✅ All files moved successfully to ./images!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
